### RAG-Based AI Legal Assistant
### Retrieval-Augmented Generation for Legal & Tax Advisory (Local, Offline Deployment)
Built with **LangChain**, **FAISS**, **Ollama**, **Streamlit**, **SQLite**, and secure user roles.

## ğŸ“Œ Overview

This project is a **local, privacy-focused AI Legal Assistant** that allows users to ask natural-language questions about **legal documents, tax laws, company policies, contracts, HR manuals**, or any uploaded PDF.

It uses **RAG (Retrieval-Augmented Generation)** to ensure answers come **directly from your own documents**, not from the modelâ€™s memory.

### ğŸ”’ 100% Local â€” No Cloud Required

* Documents never leave your PC or LAN.
* Powered by **Ollama** (local LLM) + **LangChain** pipeline.

### ğŸ‘¥ Multi-User System

* **Admin Panel:** Upload documents, rebuild index, manage users, clear database, and view logs.
* **Client Panel:** Ask questions through chat interface and receive grounded answers.

---

## ğŸš€ Features

### ğŸ”¹ Core Features

* Upload legal PDFs and build a persistent FAISS vector index
* Ask legal/tax/policy questions in natural language
* Local LLM generates answers based on retrieved document chunks
* Uses chunking, embeddings, vector similarity search

### ğŸ”¹ Admin Features

* Upload/remove documents
* Rebuild FAISS index
* Delete old files and reset knowledge base
* View logs (who asked what, when)
* Manage user accounts (add/update/delete users)

### ğŸ”¹ Client Features

* Clean chat interface
* Ask unlimited questions
* Gets answers grounded in your legal PDFs
* Safe for corporate/legal data

### ğŸ”¹ Security

* Passwords hashed using **bcrypt**
* Admin-only protected actions
* Logs stored in **SQLite** (`app_data.db`)

---

## ğŸ“ Project Folder Structure

```
rag-local-legal-assistant/
â”‚
â”œâ”€â”€ app.py                    # Main Streamlit app (Admin + Client)
â”œâ”€â”€ init_users.py             # Optional helper to create default users
â”œâ”€â”€ app_data.db               # SQLite DB (auto-created)
â”‚
â”œâ”€â”€ uploaded_files/           # PDF storage (autogenerated)
â”œâ”€â”€ vectorstores/             # FAISS index storage (autogenerated)
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ screenshots/
â”‚       â”œâ”€â”€ login.png
â”‚       â”œâ”€â”€ admin_panel.png
â”‚       â””â”€â”€ client_chat.png
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## ğŸ”§ Technology Stack

| Component              | Purpose                           |
| ---------------------- | --------------------------------- |
| **Python + Streamlit** | User interface (Admin & Client)   |
| **LangChain**          | Chunking, retrieval, RAG pipeline |
| **FAISS**              | Fast vector search                |
| **Ollama**             | Local LLM + embedding model       |
| **SQLite**             | Credentials + logs                |
| **bcrypt**             | Secure password hashing           |

---

## ğŸ” Default Login Credentials

> These are auto-created when the app starts.

### **Admin**

```
username: admin  
password: admin123
```

### **Client**

```
username: client  
password: client123
```

You can add/update/remove users in the Admin Panel.

---

## ğŸ–¼ï¸ Screenshots

### ğŸ”¹ Login Screen

![Login Screen](docs/screenshots/logininterface_UI.jpg)

### ğŸ”¹ Admin Panel

![Admin Panel](docs/screenshots/admin1_UI.jpg)
![Admin Panel](docs/screenshots/admin2_UI.jpg)

### ğŸ”¹ Client Chat Interface

![Client Chat](docs/screenshots/client_UI.jpg)

---

## âš™ï¸ Installation Guide (Beginner Friendly)

### 1ï¸âƒ£ Install Python

Install Python 3.10+ from: [https://www.python.org/downloads/](https://www.python.org/downloads/)

### 2ï¸âƒ£ Install Ollama

Download from: [https://ollama.com/download](https://ollama.com/download)

Then pull the required models:

```bash
ollama pull nomic-embed-text
ollama pull llama3
# or mistral (your choice)
```

### 3ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/daniyalmehmood2244-lab/rag-local-legal-assistant.git
cd rag-local-legal-assistant
```

### 4ï¸âƒ£ Install Python packages

```bash
pip install -r requirements.txt
```

### 5ï¸âƒ£ Run the App

Make sure Ollama server is running:

```bash
ollama serve
```

Then run Streamlit:

```bash
streamlit run app.py
```

---

## ğŸŒ Deploying on Local Wi-Fi (LAN)

This allows multiple PCs to access your AI Legal Assistant.

On HOST machine:

```bash
streamlit run app.py --server.address 0.0.0.0 --server.port 8501
```

Find your local IP:

```bash
ipconfig    # Windows
```

Clients can now open:

```
http://<your-IP>:8501
```

Example:

```
http://192.168.1.10:8501
```

---

## ğŸ§ª How It Works (RAG Pipeline)

1. **Admin uploads PDFs**
2. PDFs â†’ extracted text
3. Text is split into overlapping **chunks**
4. Chunks â†’ converted into **embeddings** (vectors)
5. Stored in **FAISS index** for fast similarity search
6. User asks a question
7. System retrieves relevant chunks
8. LLM generates answer using retrieved documents as context
9. Logs are saved in SQLite

This ensures answers are accurate and grounded in uploaded legal content.

---

## ğŸ“œ License

Add your license here (MIT recommended).
Example:

```
MIT License Â© 2025 Daniyal Mehmood
```

---

## ğŸ“« Contact

If you need help or want to collaborate:

**Email:** [daniyalmehmood2244@gmail.com](mailto:daniyalmehmood2244@gmail.com) (replace if necessary)
**GitHub:** [https://github.com/daniyalmehmood2244-lab](https://github.com/daniyalmehmood2244-lab)

---

## â­ If you find this project useful

Please give the repo a **star** â­ on GitHub â€” it helps a lot!

